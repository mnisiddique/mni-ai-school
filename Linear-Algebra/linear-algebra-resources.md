## What to learn in linear algebra in basic level?

1. Scalars, Vectors, Matrices, Tensors
   Note: Represent data (features, images, inputs/outputs)

2. Matrix Multiplication
   Note: Core of neural network computations

3. Dot Product & Vector Projections
   Note: Similarity (used in embeddings, cosine similarity, etc.)

4. Matrix Transpose, Identity, Inverse
   Note: Useful in backpropagation and optimization

5. Linear Transformations
   Note: Understand how models reshape data

6. Eigenvalues and Eigenvectors
   Note: PCA, dimensionality reduction

7. Norms (L1, L2)
   Note: Regularization and measuring distances

8. Systems of Linear Equations
   Note: Optimization, solvability of models

## What to learn in linear algebra in advance level?

1. Orthogonality, basis 
   Note: Useful in deeper understanding

2.  Rank, null space     
    Note: Optimization/solving matrix equations 

3. LU/QR Decomposition
  Note: Useful in certain optimization algorithms

4. Spectral Theorem     
   Note: Rarely used directly, but good to know 

5. Tensor operations    
   Note: Needed for deep learning (e.g., PyTorch, TensorFlow)


## What coding practice need to do for linear algebra
1. Matrix operations (np.dot, np.matmul, np.linalg.inv)

2. Vector norms (np.linalg.norm)

3. Eigen decomposition (np.linalg.eig)

4. Singular Value Decomposition (np.linalg.svd)

Note: All these are available in python's numpy library. And also learn following as more advance tools


1. Calculus (for optimization, gradients, backprop)

2. Probability & Statistics (for models, inference)

3. Optimization (gradient descent, convex functions)

4. Python with NumPy, pandas, and frameworks like PyTorch/TensorFlow



